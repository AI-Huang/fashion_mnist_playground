{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "### Team Members\n",
    "1. Zac Wellmer : resnet code, resnet tests, and report\n",
    "2. Kelly Huang : report\n",
    "\n",
    "We explore the effects of both training and test accuracy and cross entropy loss on two variations of the fashion MNIST dataset. First we construct a baseline to compare against. The baseline is the results obtained from the original dataset. We then compare to performance obtained by training on a dataset where lables are assigned based on samples from a uniform distribution. On top of this we will look at how some network parameters and hyperparameters influence this process. This is an interesting experiment because of the potential to demonstrate a neural network fitting to entirely noisey data through brute-force. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The dataset we will be working with is Fashion-MNIST from Zalando's article images. Fashion-MNIST contains 60,000 examples broken down to 50,000 train examples and 10,000 test examples. However, for the course of this experiment we had to shrink our training set size 10k to produce results similar to the rethinking generalization results[3]. As suggested  by [3] in their torch implementation[4] inputs found in datasets like MNIST/Fashion MNIST contain images very similar in pixel space so assigning random labels makes it difficult to fit. They[4] suggest that it might be possible if training is run for many more epochs but as we do not have access to many GPUs this is not as realistic for us to test. Each example is a 28 by 28 gray scale image associated with 1 of 10 classes. \n",
    "\n",
    "Each training and test example is assigned to one of the following labels:\n",
    "\n",
    "0. T-shirt/top\n",
    "1. Trouser\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandal\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle boot \n",
    "\n",
    "Below is a sample image of the fashion mnist dataset[1]\n",
    "![fashion mnist](fashion_mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures\n",
    "We focus our experiments on 3 different sizes of residual networks. The reason we look at 3 different sizes of residual networks is to measure the effects of model capacity on ability to brute force a dataset with randomly permuted labels\n",
    "\n",
    "1) 50 layer resnet\n",
    "\n",
    "2) 101 layer resnet\n",
    "\n",
    "3) 200 layer resnet\n",
    "\n",
    "Below is a sample image of a 34 layer residual network[1]\n",
    "![residual network](resnet.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "All graphs below were generated with the help of tensorboard[3]\n",
    "\n",
    "We compare accuracy and cross entropy loss on resnet50, resnet 101, and resnet200 with uniform random labels and original labels.\n",
    "\n",
    "\n",
    "| orange        | blue         | red  |\n",
    "| :-------------: | :-------------: | :-----: | :-----: |\n",
    "| random labels train      | random labels test | normal labels train |\n",
    "\n",
    "### resnet 200 acuracy\n",
    "![res200_rand_accuracy](res200_accuracy.png)\n",
    "\n",
    "### resnet 200 cross entropy loss\n",
    "![res200_rand_cross_entropy](res200_cross_entropy.png)\n",
    "\n",
    "| pink        | green         | gray  |\n",
    "| :-------------: | :-------------: | :-----: | :-----: |\n",
    "| random labels train      | random labels test | normal labels train |\n",
    "\n",
    "### resnet 101 acuracy\n",
    "![res101_rand_accuracy](res101_accuracy.png)\n",
    "\n",
    "### resnet 101 cross entropy loss\n",
    "![res101_rand_cross_entropy](res101_cross_entropy.png)\n",
    "\n",
    "| blue        | red         | cyan |\n",
    "| :-------------: | :-------------: | :-----: | :-----: |\n",
    "| random labels train      | random labels test | normal labels train |\n",
    "\n",
    "### resnet 50 acuracy\n",
    "![res50_rand_accuracy](res50_accuracy.png)\n",
    "\n",
    "### resnet 50 cross entropy loss\n",
    "![res50_rand_cross_entropy](res50_cross_entropy.png)\n",
    "\n",
    "| blue        | pink         | orange |\n",
    "| :-------------: | :-------------: | :-----: | :-----: |\n",
    "| random res50 train      | random res101 train | random res200 train |\n",
    "\n",
    "### resnet random train\n",
    "![random_trains](random_trains.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "Unsurprisingly the originally labeled dataset is able to converge the fastest. However it is interesting to see that while the random datasets converge slower it is still entirely possible for the resnets to memorize the datasets. Intuitively we had expected that larger difficulties would arrise during training such as not being able to converge or dramatically slower convergence. But the above results suggest that this is not the case for any of the three residual network architectures we tested. In order to get our models to fit to the random noise data we did not to change any hyper parameters such as the learning rate schedule. It is also interesting to see that as soon as the fitting starts convergence happens quite quickly to exactly overfit the training data. \n",
    "\n",
    "As mentioned by the author's of [3,4] memorizing inputs for MNIST is particularly difficute because many of the inputs are nearly identical to each other in pixel space. So assigning random labels to these similar images makes the process much more difficult than training on datasets such as CIFAR10 or random pixel inputs which contain inputs that are more spread out in pixel space. In the future it could be interesting to see if leaving this process to run for \n",
    "many more epochs(on the order of hundreds) could allow for memorizing the entire Fashion MNIST da\n",
    "taset. In the case of training for many more epochs on the entire dataset we might see more profound effects of higher capacity networks. In the examples we went through it seems that a smaller 50 layer resnet was easily able to memorize the dataset and did so at a much quicker pace than the higher capacity networks. We also noticed that the memorization process for the large 200 layer network had much higher variance in terms of accuracy(notice how it jumps around noticebly more than both the 50 and 101 layer). We speculate that this would not be the case when training on a larger dataset for an extended period of time with stronger compute power. This is because the larger dataset would require more parameters to be memorized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Fashion MNIST https://www.kaggle.com/zalando-research/fashionmnist\n",
    "2. Deep Residual Learning for Image Recognition https://arxiv.org/abs/1512.03385\n",
    "3. Understanding deep learning requires rethinking generalization https://arxiv.org/abs/1611.03530\n",
    "4. https://github.com/pluskid/fitting-random-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
